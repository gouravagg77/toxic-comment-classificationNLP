{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the required file\n",
    "\n",
    "train = pd.read_csv('jigsaw-toxic-comment-classification-challenge/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying the first five rows in Train dataset\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of training dataset\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         Explanation\\nWhy the edits made under my usern...\n",
      "1         D'aww! He matches this background colour I'm s...\n",
      "2         Hey man, I'm really not trying to edit war. It...\n",
      "3         \"\\nMore\\nI can't make any real suggestions on ...\n",
      "4         You, sir, are my hero. Any chance you remember...\n",
      "                                ...                        \n",
      "159566    \":::::And for the second time of asking, when ...\n",
      "159567    You should be ashamed of yourself \\n\\nThat is ...\n",
      "159568    Spitzer \\n\\nUmm, theres no actual article for ...\n",
      "159569    And it looks like it was actually you who put ...\n",
      "159570    \"\\nAnd ... I really don't think you understand...\n",
      "Name: comment_text, Length: 159571, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Segregating both feature column and target columns\n",
    "list_sentences_train = train.iloc[:,1]\n",
    "label_train = train.iloc[:, 2:8]\n",
    "\n",
    "print(list_sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing string lib for punctuations\n",
    "\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         Explanation\\nWhy the edits made under my usern...\n",
      "1         Daww He matches this background colour Im seem...\n",
      "2         Hey man Im really not trying to edit war Its j...\n",
      "3         \\nMore\\nI cant make any real suggestions on im...\n",
      "4         You sir are my hero Any chance you remember wh...\n",
      "                                ...                        \n",
      "159566    And for the second time of asking when your vi...\n",
      "159567    You should be ashamed of yourself \\n\\nThat is ...\n",
      "159568    Spitzer \\n\\nUmm theres no actual article for p...\n",
      "159569    And it looks like it was actually you who put ...\n",
      "159570    \\nAnd  I really dont think you understand  I c...\n",
      "Name: comment_text, Length: 159571, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Removing punctuations from text\n",
    "\n",
    "def remove_punctuations(sentence):\n",
    "  txt_nopunct = \"\".join([c for c in sentence if c not in string.punctuation])\n",
    "  return txt_nopunct\n",
    "\n",
    "list_sentences_train = list_sentences_train.apply(lambda x:remove_punctuations(x))\n",
    "\n",
    "print(list_sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         [explanation, edits, made, username, hardcore,...\n",
      "1         [daww, matches, background, colour, im, seemin...\n",
      "2         [hey, man, im, really, trying, edit, war, guy,...\n",
      "3         [cant, make, real, suggestions, improvement, w...\n",
      "4                [sir, hero, chance, remember, page, thats]\n",
      "                                ...                        \n",
      "159566    [second, time, asking, view, completely, contr...\n",
      "159567    [ashamed, horrible, thing, put, talk, page, 12...\n",
      "159568    [spitzer, umm, theres, actual, article, prosti...\n",
      "159569    [looks, like, actually, put, speedy, first, ve...\n",
      "159570    [really, dont, think, understand, came, idea, ...\n",
      "Name: comment_text, Length: 159571, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing and removing stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def tokenize_sentences(sentence):\n",
    "  tokens = word_tokenize(sentence)\n",
    "  filtered_sentence = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "  return filtered_sentence\n",
    "\n",
    "list_sentences_train = list_sentences_train.apply(lambda x : tokenize_sentences(x))\n",
    "\n",
    "print(list_sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         explanation edits made username hardcore metal...\n",
      "1         daww match background colour im seemingly stuc...\n",
      "2         hey man im really trying edit war guy constant...\n",
      "3         cant make real suggestion improvement wondered...\n",
      "4                       sir hero chance remember page thats\n",
      "                                ...                        \n",
      "159566    second time asking view completely contradicts...\n",
      "159567       ashamed horrible thing put talk page 128611993\n",
      "159568    spitzer umm there actual article prostitution ...\n",
      "159569    look like actually put speedy first version de...\n",
      "159570    really dont think understand came idea bad rig...\n",
      "Name: comment_text, Length: 159571, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Lemmatizing\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(tokenized_text):\n",
    "  text = \" \".join([wn.lemmatize(word) for word in tokenized_text])\n",
    "  return text\n",
    "\n",
    "list_sentences_train_lemma = list_sentences_train.apply(lambda x: lemmatization(x))\n",
    "\n",
    "print(list_sentences_train_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization\n",
    "#Process of encoding text as integers to create Feature Vectors\n",
    "#Feature Vector: vector of numerical features that represent an object\n",
    "\n",
    "# Instantiate the vectorizer\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=30000)\n",
    "\n",
    "# fit and transform on it the training features\n",
    "word_vectorizer.fit(list_sentences_train_lemma)\n",
    "X_train_word_features = word_vectorizer.transform(list_sentences_train_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 29798)\t0.21370421658218852\n",
      "  (0, 28855)\t0.22915641879892687\n",
      "  (0, 28537)\t0.24502732596469912\n",
      "  (0, 28188)\t0.1521339619561107\n",
      "  (0, 28052)\t0.19434663672764638\n",
      "  (0, 26365)\t0.16917491539311266\n",
      "  (0, 26126)\t0.09686503344819589\n",
      "  (0, 22615)\t0.1632751731066122\n",
      "  (0, 22559)\t0.2551043308922774\n",
      "  (0, 22254)\t0.1504893135303345\n",
      "  (0, 19479)\t0.08711972514281327\n",
      "  (0, 18276)\t0.13282644778226682\n",
      "  (0, 17164)\t0.32222275040479137\n",
      "  (0, 13595)\t0.11215758229293775\n",
      "  (0, 12560)\t0.2807022467718628\n",
      "  (0, 11600)\t0.2505527407985963\n",
      "  (0, 10450)\t0.2001534511033996\n",
      "  (0, 10324)\t0.2491177485248909\n",
      "  (0, 10200)\t0.18454401523372843\n",
      "  (0, 9261)\t0.13497106289643243\n",
      "  (0, 8841)\t0.10220808487793585\n",
      "  (0, 8796)\t0.31747993851804324\n",
      "  (0, 6017)\t0.2821920770988895\n",
      "  (1, 28099)\t0.17682068642782556\n",
      "  (1, 26504)\t0.14986144835669837\n",
      "  :\t:\n",
      "  (159568, 21110)\t0.3929105451235221\n",
      "  (159568, 7375)\t0.4388896373537555\n",
      "  (159568, 5124)\t0.3428512093895419\n",
      "  (159568, 2944)\t0.10496360798136958\n",
      "  (159568, 1739)\t0.24224340027771488\n",
      "  (159569, 28328)\t0.4227822138325273\n",
      "  (159569, 24999)\t0.4411368775849789\n",
      "  (159569, 16172)\t0.5482029933718445\n",
      "  (159569, 15925)\t0.2530937230897548\n",
      "  (159569, 7907)\t0.3546232262335322\n",
      "  (159569, 1741)\t0.3692057254538172\n",
      "  (159570, 27626)\t0.20884792573421615\n",
      "  (159570, 26592)\t0.1480837089915277\n",
      "  (159570, 22754)\t0.18207118646917816\n",
      "  (159570, 22662)\t0.3025196609122195\n",
      "  (159570, 21753)\t0.18162369141919582\n",
      "  (159570, 15300)\t0.2269653757384711\n",
      "  (159570, 14198)\t0.22037770635260379\n",
      "  (159570, 13496)\t0.3699689017059218\n",
      "  (159570, 12809)\t0.2954759077389117\n",
      "  (159570, 8841)\t0.1394776566644726\n",
      "  (159570, 6358)\t0.24305227875029045\n",
      "  (159570, 5025)\t0.2453828570859325\n",
      "  (159570, 3451)\t0.3874452403312179\n",
      "  (159570, 3346)\t0.40457499717123235\n"
     ]
    }
   ],
   "source": [
    "#Vectored matrix\n",
    "\n",
    "print(X_train_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into training and testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_word_features, label_train, test_size= 0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aggarwal\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy score for class toxic is 0.9548865689803255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aggarwal\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28750   177]\n",
      " [ 1224  1764]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     28927\n",
      "           1       0.91      0.59      0.72      2988\n",
      "\n",
      "    accuracy                           0.96     31915\n",
      "   macro avg       0.93      0.79      0.85     31915\n",
      "weighted avg       0.95      0.96      0.95     31915\n",
      "\n",
      "CV Accuracy score for class severe_toxic is 0.9904587351126126\n",
      "[[31558    54]\n",
      " [  237    66]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     31612\n",
      "           1       0.55      0.22      0.31       303\n",
      "\n",
      "    accuracy                           0.99     31915\n",
      "   macro avg       0.77      0.61      0.65     31915\n",
      "weighted avg       0.99      0.99      0.99     31915\n",
      "\n",
      "CV Accuracy score for class obscene is 0.9763740167306384\n",
      "[[30191   100]\n",
      " [  620  1004]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     30291\n",
      "           1       0.91      0.62      0.74      1624\n",
      "\n",
      "    accuracy                           0.98     31915\n",
      "   macro avg       0.94      0.81      0.86     31915\n",
      "weighted avg       0.98      0.98      0.98     31915\n",
      "\n",
      "CV Accuracy score for class threat is 0.9971799213439123\n",
      "[[31815    10]\n",
      " [   79    11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     31825\n",
      "           1       0.52      0.12      0.20        90\n",
      "\n",
      "    accuracy                           1.00     31915\n",
      "   macro avg       0.76      0.56      0.60     31915\n",
      "weighted avg       1.00      1.00      1.00     31915\n",
      "\n",
      "CV Accuracy score for class insult is 0.9695039795599987\n",
      "[[30242   170]\n",
      " [  764   739]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     30412\n",
      "           1       0.81      0.49      0.61      1503\n",
      "\n",
      "    accuracy                           0.97     31915\n",
      "   macro avg       0.89      0.74      0.80     31915\n",
      "weighted avg       0.97      0.97      0.97     31915\n",
      "\n",
      "CV Accuracy score for class identity_hate is 0.9919314425859203\n",
      "[[31613    25]\n",
      " [  231    46]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     31638\n",
      "           1       0.65      0.17      0.26       277\n",
      "\n",
      "    accuracy                           0.99     31915\n",
      "   macro avg       0.82      0.58      0.63     31915\n",
      "weighted avg       0.99      0.99      0.99     31915\n",
      "\n",
      "Total average Auccarcy score is 0.9800557773855679\n"
     ]
    }
   ],
   "source": [
    "#Training the model and evaluating it \n",
    "#Logistic Regression ,storing all the six models (trained on each target column) in classifier array\n",
    "\n",
    "CLASSES = ['toxic','severe_toxic','obscene', 'threat', 'insult', 'identity_hate']\n",
    "classifier=[]\n",
    "auc = []\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    #Taking each column label each time to train the classifier on it\n",
    "    train_target = y_train[class_name]\n",
    "    test_target = y_test[class_name]\n",
    "    \n",
    "    #Initializing Classifier\n",
    "    classifier_logistic= LogisticRegression()\n",
    "    classifier.append(classifier_logistic)\n",
    "    \n",
    "    cv_score = np.mean(cross_val_score(classifier_logistic, X_train, train_target, cv=5, scoring='accuracy'))\n",
    "    print('CV Accuracy score for class {} is {}'.format(class_name, cv_score))\n",
    "    auc.append(cv_score)\n",
    "    \n",
    "    classifier_logistic.fit(X_train, train_target)\n",
    "    y_pred = classifier_logistic.predict(X_test)\n",
    "\n",
    "    print(confusion_matrix(test_target, y_pred))\n",
    "    print(classification_report(test_target, y_pred))\n",
    "\n",
    "print('Total average Auccarcy score is {}'.format(np.mean(auc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing the same preprocessing steps on unseen data\n",
    "test = pd.read_csv('jigsaw-toxic-comment-classification-challenge/test.csv')\n",
    "\n",
    "#Taking commennts Column\n",
    "list_sentences_test = test.iloc[:,1]\n",
    "\n",
    "#Removing Punctations\n",
    "list_sentences_test = list_sentences_test.apply(lambda x:remove_punctuations(x)) \n",
    "\n",
    "#Tokenizing and removing stopwords\n",
    "list_sentences_test = list_sentences_test.apply(lambda x : tokenize_sentences(x))\n",
    "\n",
    "#Lemmatizing\n",
    "list_sentences_test_lemma = list_sentences_test.apply(lambda x: lemmatization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         yo bitch ja rule succesful youll ever whats ha...\n",
      "1                                        rfc title fine imo\n",
      "2                              source zawe ashton lapland —\n",
      "3         look back source information updated correct f...\n",
      "4                             dont anonymously edit article\n",
      "                                ...                        \n",
      "153159              totally agree stuff nothing toolongcrap\n",
      "153160    throw field home plate get faster throwing cut...\n",
      "153161    okinotorishima category see change agree corre...\n",
      "153162    one founding nation eu germany law return quit...\n",
      "153163    stop already bullshit welcome im fool think ki...\n",
      "Name: comment_text, Length: 153164, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(list_sentences_test_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization\n",
    "X_test_word_features = word_vectorizer.transform(list_sentences_test_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 29812)\t0.13397809784610532\n",
      "  (0, 29783)\t0.16642249784669857\n",
      "  (0, 29610)\t0.10456603197974933\n",
      "  (0, 28925)\t0.22931712381462893\n",
      "  (0, 28887)\t0.12754911466212576\n",
      "  (0, 27977)\t0.16246453661751956\n",
      "  (0, 27328)\t0.2367330103048625\n",
      "  (0, 26732)\t0.07776576681974647\n",
      "  (0, 26590)\t0.08833771056432077\n",
      "  (0, 25659)\t0.22536846112051273\n",
      "  (0, 24488)\t0.19861726358400716\n",
      "  (0, 24101)\t0.12999190427356352\n",
      "  (0, 23178)\t0.15029102890662024\n",
      "  (0, 23072)\t0.18838316421100115\n",
      "  (0, 22754)\t0.09215444645906201\n",
      "  (0, 20863)\t0.18125281898577186\n",
      "  (0, 18628)\t0.23367617846788133\n",
      "  (0, 17883)\t0.13813715217326333\n",
      "  (0, 16558)\t0.11981680272624233\n",
      "  (0, 15925)\t0.06906446455843968\n",
      "  (0, 15327)\t0.1833768001045843\n",
      "  (0, 14662)\t0.36421657175902555\n",
      "  (0, 12647)\t0.20583017251626448\n",
      "  (0, 12359)\t0.11526352669361355\n",
      "  (0, 10326)\t0.13982856522366022\n",
      "  :\t:\n",
      "  (153162, 15643)\t0.12112109949347674\n",
      "  (153162, 14835)\t0.13301797657289877\n",
      "  (153162, 14587)\t0.29929478504856766\n",
      "  (153162, 14576)\t0.09973665722378781\n",
      "  (153162, 11760)\t0.30736860473493194\n",
      "  (153162, 11144)\t0.18008372885709192\n",
      "  (153162, 9941)\t0.1692872242881929\n",
      "  (153162, 8101)\t0.17185807585687357\n",
      "  (153162, 6845)\t0.16166854531244879\n",
      "  (153162, 5843)\t0.15109912035851913\n",
      "  (153162, 2477)\t0.22128116015010946\n",
      "  (153162, 2422)\t0.20569829099593667\n",
      "  (153162, 2420)\t0.1683653204268721\n",
      "  (153162, 2219)\t0.258249279043605\n",
      "  (153162, 2214)\t0.12879376716719163\n",
      "  (153162, 1917)\t0.19086630694985343\n",
      "  (153162, 1741)\t0.09417319849010829\n",
      "  (153163, 28824)\t0.29289839478595686\n",
      "  (153163, 26592)\t0.20743789491801798\n",
      "  (153163, 25402)\t0.2693208871919203\n",
      "  (153163, 20250)\t0.5154757456509423\n",
      "  (153163, 15300)\t0.3179365244367231\n",
      "  (153163, 13595)\t0.21440200555080813\n",
      "  (153163, 11013)\t0.43864936507093866\n",
      "  (153163, 4810)\t0.43984764000651816\n"
     ]
    }
   ],
   "source": [
    "print(X_test_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 30000)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_word_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment -- Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time., \n",
      "toxic [1]\n",
      "severe_toxic [0]\n",
      "obscene [1]\n",
      "threat [0]\n",
      "insult [1]\n",
      "identity_hate [0]\n"
     ]
    }
   ],
   "source": [
    "#testing on first comment in test dataset\n",
    "\n",
    "CLASSES = ['toxic','severe_toxic','obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "print(\"comment -- {} \".format(test.iloc[0,1]))\n",
    "i=0\n",
    "for class_name in CLASSES:\n",
    "    #Taking each column label each time to train the classifier on it\n",
    "    y_pred = classifier[i].predict(X_test_word_features[0])\n",
    "    i=i+1\n",
    "    print(class_name,y_pred)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on all the rows in test dataset and storing it as a DataFrame\n",
    "columns = ['comment_text'] + CLASSES\n",
    "df = pd.DataFrame(columns=columns)\n",
    "rows = 138484\n",
    "\n",
    "def get_predictions(i):\n",
    "    predict=[test.iloc[i,1]]\n",
    "    j=0\n",
    "    for class_name in CLASSES:\n",
    "        y_pred = classifier[j].predict(X_test_word_features[i])\n",
    "        j=j+1\n",
    "        predict.append(y_pred)\n",
    "    return predict\n",
    "\n",
    "for i in range(0,test.shape[0]+1):\n",
    "    p = get_predictions(i)\n",
    "    df.loc[i]=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138479</th>\n",
       "      <td>Wikipedia:Criticism#Avoid_sections_and_article...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138480</th>\n",
       "      <td>(night after night that wanker is causing trou...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138481</th>\n",
       "      <td>.The Macanese Yu-7 killed 10 officers during a...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138482</th>\n",
       "      <td>==Oi!!== \\n\\n Please do not remove me abusing ...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138483</th>\n",
       "      <td>\" \\n\\n == I think you got confused == \\n\\n You...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138484 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text toxic severe_toxic  \\\n",
       "0       Yo bitch Ja Rule is more succesful then you'll...   [1]          [0]   \n",
       "1       == From RfC == \\n\\n The title is fine as it is...   [0]          [0]   \n",
       "2       \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...   [0]          [0]   \n",
       "3       :If you have a look back at the source, the in...   [0]          [0]   \n",
       "4               I don't anonymously edit articles at all.   [0]          [0]   \n",
       "...                                                   ...   ...          ...   \n",
       "138479  Wikipedia:Criticism#Avoid_sections_and_article...   [0]          [0]   \n",
       "138480  (night after night that wanker is causing trou...   [1]          [0]   \n",
       "138481  .The Macanese Yu-7 killed 10 officers during a...   [0]          [0]   \n",
       "138482  ==Oi!!== \\n\\n Please do not remove me abusing ...   [1]          [0]   \n",
       "138483  \" \\n\\n == I think you got confused == \\n\\n You...   [0]          [0]   \n",
       "\n",
       "       obscene threat insult identity_hate  \n",
       "0          [1]    [0]    [1]           [0]  \n",
       "1          [0]    [0]    [0]           [0]  \n",
       "2          [0]    [0]    [0]           [0]  \n",
       "3          [0]    [0]    [0]           [0]  \n",
       "4          [0]    [0]    [0]           [0]  \n",
       "...        ...    ...    ...           ...  \n",
       "138479     [0]    [0]    [0]           [0]  \n",
       "138480     [0]    [0]    [0]           [0]  \n",
       "138481     [0]    [0]    [0]           [0]  \n",
       "138482     [0]    [0]    [0]           [0]  \n",
       "138483     [0]    [0]    [0]           [0]  \n",
       "\n",
       "[138484 rows x 7 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
